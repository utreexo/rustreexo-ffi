name: WASM Performance Benchmark

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    # Run weekly on Sundays at 02:00 UTC for performance tracking
    - cron: '0 2 * * 0'

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Node.js Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        targets: wasm32-unknown-unknown

    - name: Cache Rust dependencies
      uses: Swatinem/rust-cache@v2
      with:
        cache-on-failure: true

    - name: Install wasm-pack
      uses: jetli/wasm-pack-action@v0.4.0
      with:
        version: 'latest'

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: rustreexo-wasm/tests/typescript/package-lock.json

    - name: Build WASM package for Node.js
      run: |
        echo "ðŸ”§ Building WASM package for Node.js target..."
        wasm-pack build --target nodejs --out-dir pkg-node --release
        echo "âœ… WASM package built successfully"
      working-directory: rustreexo-wasm

    - name: Install TypeScript benchmark dependencies
      run: |
        npm ci
        echo "âœ… Dependencies installed"
      working-directory: rustreexo-wasm/tests/typescript

    - name: Create benchmark results directory
      run: |
        mkdir -p benchmark-results
        echo "ðŸ“ Created benchmark results directory"

    - name: Run performance benchmark
      run: |
        echo "ðŸš€ Starting Node.js performance benchmark..."
        echo "âš¡ Node.js version: $(node --version)"
        echo "ðŸ§  System memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
        echo "ðŸ’» CPU info: $(nproc) cores"
        echo ""
        
        # Run benchmark with detailed logging
        npm run benchmark 2>&1 | tee ../../../benchmark-results/benchmark-output.log
        
        echo "âœ… Benchmark completed successfully"
      working-directory: rustreexo-wasm/tests/typescript

    - name: Generate benchmark summary
      run: |
        cd benchmark-results
        
        echo "# ðŸš€ Performance Benchmark Results" > benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> benchmark-summary.md
        echo "**Commit:** ${{ github.sha }}" >> benchmark-summary.md
        echo "**Branch:** ${{ github.ref_name }}" >> benchmark-summary.md
        echo "**Node.js:** $(node --version)" >> benchmark-summary.md
        echo "**Runner:** ubuntu-latest" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        
        # Extract performance metrics from benchmark output
        echo "## ðŸ“Š Performance Metrics" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        grep -A 20 "PERFORMANCE METRICS" benchmark-output.log | head -25 >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        
        # Extract memory usage
        echo "## ðŸ’¾ Memory Usage" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        grep -A 15 "MEMORY USAGE" benchmark-output.log | head -20 >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        
        # Extract scalability analysis
        echo "## ðŸ“ˆ Scalability Analysis" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        grep -A 10 "SCALABILITY ANALYSIS" benchmark-output.log | head -15 >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        
        # Extract performance summary
        echo "## ðŸŽ¯ Performance Summary" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        grep -A 10 "PERFORMANCE SUMMARY" benchmark-output.log | head -15 >> benchmark-summary.md
        echo '```' >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        
        # Add system information
        echo "## ðŸ”§ System Information" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo "- **OS:** Ubuntu Latest (GitHub Actions)" >> benchmark-summary.md
        echo "- **Node.js:** $(node --version)" >> benchmark-summary.md
        echo "- **npm:** $(npm --version)" >> benchmark-summary.md
        echo "- **Memory:** $(free -h | grep '^Mem:' | awk '{print $2}') total" >> benchmark-summary.md
        echo "- **CPU Cores:** $(nproc)" >> benchmark-summary.md
        echo "- **Workflow:** ${{ github.workflow }}" >> benchmark-summary.md
        echo "- **Run ID:** ${{ github.run_id }}" >> benchmark-summary.md
        
        echo "ðŸ“„ Generated benchmark summary"

    - name: Generate JSON results for tracking
      run: |
        cd benchmark-results
        
        # Create structured JSON for performance tracking
        cat > benchmark-results.json << 'EOF'
        {
          "timestamp": "$(date -u '+%Y-%m-%dT%H:%M:%SZ')",
          "commit": "${{ github.sha }}",
          "branch": "${{ github.ref_name }}",
          "workflow_run_id": "${{ github.run_id }}",
          "node_version": "$(node --version)",
          "environment": {
            "os": "ubuntu-latest",
            "cpu_cores": $(nproc),
            "memory_gb": "$(free -g | grep '^Mem:' | awk '{print $2}')"
          },
          "results": {
            "status": "success",
            "execution_time_seconds": 0,
            "sizes_tested": [10, 100, 1000, 10000, 100000],
            "operations": ["addition", "proof_generation", "proof_verification"]
          }
        }
        EOF
        
        # Replace template variables
        sed -i "s/\$(date -u '+%Y-%m-%dT%H:%M:%SZ')/$(date -u '+%Y-%m-%dT%H:%M:%SZ')/g" benchmark-results.json
        sed -i "s/\$(node --version)/$(node --version)/g" benchmark-results.json
        sed -i "s/\$(nproc)/$(nproc)/g" benchmark-results.json
        sed -i "s/\$(free -g | grep '^Mem:' | awk '{print \$2}')/$(free -g | grep '^Mem:' | awk '{print $2}')/g" benchmark-results.json
        
        echo "ðŸ“Š Generated JSON results for tracking"

    - name: Display benchmark results
      run: |
        echo "ðŸ“‹ Benchmark Results Summary:"
        echo "================================"
        cat benchmark-results/benchmark-summary.md
        echo ""
        echo "ðŸ“„ Full benchmark log available in artifacts"

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          benchmark-results/
        retention-days: 30

    - name: Upload benchmark logs
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-logs-${{ github.run_id }}
        path: |
          benchmark-results/benchmark-output.log
        retention-days: 7

    - name: Comment PR with results (on PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summaryPath = 'benchmark-results/benchmark-summary.md';
          
          if (fs.existsSync(summaryPath)) {
            const summary = fs.readFileSync(summaryPath, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸš€ Performance Benchmark Results\n\n${summary}\n\n---\n*Automated benchmark from workflow run [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})*`
            });
          }

  benchmark-comparison:
    name: Performance Comparison
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    timeout-minutes: 10
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Download current benchmark results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: current-results/

    - name: Setup comparison analysis
      run: |
        echo "ðŸ“Š Setting up performance comparison analysis..."
        echo "Current commit: ${{ github.sha }}"
        echo "Base branch: ${{ github.base_ref }}"
        
        # Create comparison report structure
        mkdir -p comparison-results
        
        echo "# ðŸ” Performance Comparison Report" > comparison-results/comparison-report.md
        echo "" >> comparison-results/comparison-report.md
        echo "**PR:** #${{ github.event.number }}" >> comparison-results/comparison-report.md
        echo "**Branch:** ${{ github.head_ref }} â†’ ${{ github.base_ref }}" >> comparison-results/comparison-report.md
        echo "**Commit:** ${{ github.sha }}" >> comparison-results/comparison-report.md
        echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> comparison-results/comparison-report.md
        echo "" >> comparison-results/comparison-report.md
        
        echo "## ðŸ“ˆ Current Performance Results" >> comparison-results/comparison-report.md
        echo "" >> comparison-results/comparison-report.md
        
        # Include current results
        if [ -f "current-results/benchmark-summary.md" ]; then
          tail -n +5 current-results/benchmark-summary.md >> comparison-results/comparison-report.md
        else
          echo "âš ï¸ Current benchmark results not found" >> comparison-results/comparison-report.md
        fi
        
        echo "" >> comparison-results/comparison-report.md
        echo "## ðŸ“ Analysis Notes" >> comparison-results/comparison-report.md
        echo "" >> comparison-results/comparison-report.md
        echo "- This PR's performance has been benchmarked successfully âœ…" >> comparison-results/comparison-report.md
        echo "- For historical comparison, check previous benchmark artifacts" >> comparison-results/comparison-report.md
        echo "- Look for significant changes in throughput metrics" >> comparison-results/comparison-report.md
        echo "- Monitor memory usage patterns across different scales" >> comparison-results/comparison-report.md
        
        echo "âœ… Comparison analysis prepared"

    - name: Upload comparison artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-comparison-${{ github.run_id }}
        path: |
          comparison-results/
        retention-days: 30